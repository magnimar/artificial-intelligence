{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"car_prices.csv\", chunksize=10000)\n",
    "\n",
    "cat_columns = [\n",
    "    'make', \n",
    "    'model', \n",
    "    'trim', \n",
    "    'body', \n",
    "    'transmission', \n",
    "    'state', \n",
    "    'color', \n",
    "    'interior', \n",
    "]\n",
    "\n",
    "numeric_column = [\n",
    "    'year', \n",
    "    'condition', \n",
    "    'odometer', \n",
    "    'mmr', \n",
    "    'sellingprice',\n",
    "    'days_since_sale'\n",
    "]\n",
    "\n",
    "# drop a column\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(df, columns):\n",
    "    \"\"\"\n",
    "    One-hot encodes specified columns in a pandas DataFrame.\n",
    "    \"\"\"\n",
    "    for column in columns:\n",
    "        dummies = pd.get_dummies(df[column], prefix=column)\n",
    "        df = pd.concat([df, dummies], axis=1)\n",
    "        df.drop(column, axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "def normalize(df, columns):\n",
    "    \"\"\"\n",
    "    Normalizes specified columns in a pandas DataFrame between 0 and 1.\n",
    "    \"\"\"\n",
    "    for column in columns:\n",
    "        df[column] = (df[column] - df[column].min()) / (df[column].max() - df[column].min())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            year  condition  odometer       mmr  sellingprice  days_since_sale  make_Acura  make_Audi  make_BMW  make_Bentley  make_Buick  make_Cadillac  make_Chevrolet  make_Chrysler  make_Dodge  make_FIAT  make_Ferrari  make_Ford  make_GMC  make_Geo  make_HUMMER  make_Honda  make_Hyundai  make_Infiniti  make_Isuzu  make_Jaguar  make_Jeep  make_Kia  make_Land Rover  make_Lexus  make_Lincoln  make_MINI  make_Maserati  make_Mazda  make_Mercedes-Benz  make_Mercury  make_Mitsubishi  make_Nissan  make_Oldsmobile  make_Pontiac  make_Porsche  make_Ram  make_Saab  make_Saturn  make_Scion  make_Subaru  make_Suzuki  make_Toyota  make_Volkswagen  make_Volvo  ...  trim_car signature  trim_cherokee 4x4  trim_cxl  trim_fwd lt  trim_gle  trim_lt w/2lt  trim_sierra 4x2 sl  trim_sub 4x4 v8  trim_xl  model_300ZX  model_Aura Hybrid  model_GS 460  model_Golf GTI  model_MKZ Hybrid  model_Tribute Hybrid  model_g2500  model_golf  model_i-Series  model_tahoe  trim_2+2  trim_2.5 I  trim_2.5 XT  \\\n",
      "0       1.000000      1.000  0.016638  0.137439      0.139605         0.371115           0          0         0           0.0           0              0               0              0           0        0.0           0.0          0         0       0.0            0           0             0              0         0.0            0          0         1                0           0             0          0            0.0           0                   0             0                0            0              0.0             0             0       0.0          0            0           0            0            0            0                0           0  ...                 0.0                0.0       0.0          0.0       0.0            0.0                 0.0              0.0      0.0          0.0                0.0           0.0             0.0               0.0                   0.0          0.0         0.0             0.0          0.0       0.0         0.0          0.0   \n",
      "1       1.000000      1.000  0.009392  0.139453      0.139605         0.371115           0          0         0           0.0           0              0               0              0           0        0.0           0.0          0         0       0.0            0           0             0              0         0.0            0          0         1                0           0             0          0            0.0           0                   0             0                0            0              0.0             0             0       0.0          0            0           0            0            0            0                0           0  ...                 0.0                0.0       0.0          0.0       0.0            0.0                 0.0              0.0      0.0          0.0                0.0           0.0             0.0               0.0                   0.0          0.0         0.0             0.0          0.0       0.0         0.0          0.0   \n",
      "2       0.965517      0.875  0.001330  0.213962      0.194800         0.318099           0          0         1           0.0           0              0               0              0           0        0.0           0.0          0         0       0.0            0           0             0              0         0.0            0          0         0                0           0             0          0            0.0           0                   0             0                0            0              0.0             0             0       0.0          0            0           0            0            0            0                0           0  ...                 0.0                0.0       0.0          0.0       0.0            0.0                 0.0              0.0      0.0          0.0                0.0           0.0             0.0               0.0                   0.0          0.0         0.0             0.0          0.0       0.0         0.0          0.0   \n",
      "3       1.000000      0.775  0.014281  0.184427      0.180189         0.292505           0          0         0           0.0           0              0               0              0           0        0.0           0.0          0         0       0.0            0           0             0              0         0.0            0          0         0                0           0             0          0            0.0           0                   0             0                0            0              0.0             0             0       0.0          0            0           0            0            0            0                0           1  ...                 0.0                0.0       0.0          0.0       0.0            0.0                 0.0              0.0      0.0          0.0                0.0           0.0             0.0               0.0                   0.0          0.0         0.0             0.0          0.0       0.0         0.0          0.0   \n",
      "4       0.965517      0.825  0.002640  0.442860      0.435061         0.367459           0          0         1           0.0           0              0               0              0           0        0.0           0.0          0         0       0.0            0           0             0              0         0.0            0          0         0                0           0             0          0            0.0           0                   0             0                0            0              0.0             0             0       0.0          0            0           0            0            0            0                0           0  ...                 0.0                0.0       0.0          0.0       0.0            0.0                 0.0              0.0      0.0          0.0                0.0           0.0             0.0               0.0                   0.0          0.0         0.0             0.0          0.0       0.0         0.0          0.0   \n",
      "...          ...        ...       ...       ...           ...              ...         ...        ...       ...           ...         ...            ...             ...            ...         ...        ...           ...        ...       ...       ...          ...         ...           ...            ...         ...          ...        ...       ...              ...         ...           ...        ...            ...         ...                 ...           ...              ...          ...              ...           ...           ...       ...        ...          ...         ...          ...          ...          ...              ...         ...  ...                 ...                ...       ...          ...       ...            ...                 ...              ...      ...          ...                ...           ...             ...               ...                   ...          ...         ...             ...          ...       ...         ...          ...   \n",
      "109995  0.967742      0.725  0.010082  0.305102      0.225709         0.846995           0          0         0           0.0           0              0               0              0           0        0.0           0.0          1         0       0.0            0           0             0              0         0.0            0          0         0                0           0             0          0            0.0           0                   0             0                0            0              0.0             0             0       0.0          0            0           0            0            0            0                0           0  ...                 0.0                0.0       0.0          0.0       0.0            0.0                 0.0              0.0      0.0          0.0                0.0           0.0             0.0               0.0                   0.0          0.0         0.0             0.0          0.0       0.0         0.0          0.0   \n",
      "109996  0.967742      0.725  0.026161  0.241747      0.156883         0.928962           0          0         0           0.0           0              0               0              0           0        0.0           0.0          1         0       0.0            0           0             0              0         0.0            0          0         0                0           0             0          0            0.0           0                   0             0                0            0              0.0             0             0       0.0          0            0           0            0            0            0                0           0  ...                 0.0                0.0       0.0          0.0       0.0            0.0                 0.0              0.0      0.0          0.0                0.0           0.0             0.0               0.0                   0.0          0.0         0.0             0.0          0.0       0.0         0.0          0.0   \n",
      "109997  0.967742      0.775  0.026206  0.243748      0.160931         0.961749           0          0         0           0.0           0              0               0              0           0        0.0           0.0          1         0       0.0            0           0             0              0         0.0            0          0         0                0           0             0          0            0.0           0                   0             0                0            0              0.0             0             0       0.0          0            0           0            0            0            0                0           0  ...                 0.0                0.0       0.0          0.0       0.0            0.0                 0.0              0.0      0.0          0.0                0.0           0.0             0.0               0.0                   0.0          0.0         0.0             0.0          0.0       0.0         0.0          0.0   \n",
      "109998  0.967742      0.850  0.002792  0.315772      0.228745         0.961749           0          0         0           0.0           0              0               0              0           0        0.0           0.0          1         0       0.0            0           0             0              0         0.0            0          0         0                0           0             0          0            0.0           0                   0             0                0            0              0.0             0             0       0.0          0            0           0            0            0            0                0           0  ...                 0.0                0.0       0.0          0.0       0.0            0.0                 0.0              0.0      0.0          0.0                0.0           0.0             0.0               0.0                   0.0          0.0         0.0             0.0          0.0       0.0         0.0          0.0   \n",
      "109999  0.967742      0.850  0.004199  0.313104      0.228745         0.961749           0          0         0           0.0           0              0               0              0           0        0.0           0.0          1         0       0.0            0           0             0              0         0.0            0          0         0                0           0             0          0            0.0           0                   0             0                0            0              0.0             0             0       0.0          0            0           0            0            0            0                0           0  ...                 0.0                0.0       0.0          0.0       0.0            0.0                 0.0              0.0      0.0          0.0                0.0           0.0             0.0               0.0                   0.0          0.0         0.0             0.0          0.0       0.0         0.0          0.0   \n",
      "\n",
      "        trim_2.5L Blush Edition PZEV  trim_2.5L Final Edition PZEV  trim_3.6l se se  trim_4x2 reg base  trim_4x2 v8 300c srt8  trim_4x2 v8 ed bauer  trim_4x2 v8 lt  trim_4x4 4c sport  trim_4x4 cr rts  trim_4x4 v8 sle  trim_C32 AMG  trim_GL TDI  trim_Green Line  trim_Olympic Gold  trim_S 5 Passenger  trim_SH-AWD w/Advance and Entertainment Packages  trim_Touring V-6  trim_V6 R-Line  trim_VR6 Sport  trim_chevy van base  trim_city 2  trim_detective  trim_ext sle  trim_i-290 LS  trim_sDrive35is  trim_se w/nav  trim_se w/rse  trim_xlt 4x4 xlt sc  \n",
      "0                                0.0                           0.0              0.0                0.0                    0.0                   0.0             0.0                0.0              0.0              0.0           0.0          0.0              0.0                0.0                 0.0                                               0.0               0.0             0.0             0.0                  0.0          0.0             0.0           0.0            0.0              0.0            0.0            0.0                  0.0  \n",
      "1                                0.0                           0.0              0.0                0.0                    0.0                   0.0             0.0                0.0              0.0              0.0           0.0          0.0              0.0                0.0                 0.0                                               0.0               0.0             0.0             0.0                  0.0          0.0             0.0           0.0            0.0              0.0            0.0            0.0                  0.0  \n",
      "2                                0.0                           0.0              0.0                0.0                    0.0                   0.0             0.0                0.0              0.0              0.0           0.0          0.0              0.0                0.0                 0.0                                               0.0               0.0             0.0             0.0                  0.0          0.0             0.0           0.0            0.0              0.0            0.0            0.0                  0.0  \n",
      "3                                0.0                           0.0              0.0                0.0                    0.0                   0.0             0.0                0.0              0.0              0.0           0.0          0.0              0.0                0.0                 0.0                                               0.0               0.0             0.0             0.0                  0.0          0.0             0.0           0.0            0.0              0.0            0.0            0.0                  0.0  \n",
      "4                                0.0                           0.0              0.0                0.0                    0.0                   0.0             0.0                0.0              0.0              0.0           0.0          0.0              0.0                0.0                 0.0                                               0.0               0.0             0.0             0.0                  0.0          0.0             0.0           0.0            0.0              0.0            0.0            0.0                  0.0  \n",
      "...                              ...                           ...              ...                ...                    ...                   ...             ...                ...              ...              ...           ...          ...              ...                ...                 ...                                               ...               ...             ...             ...                  ...          ...             ...           ...            ...              ...            ...            ...                  ...  \n",
      "109995                           0.0                           0.0              0.0                0.0                    0.0                   0.0             0.0                0.0              0.0              0.0           0.0          0.0              0.0                0.0                 0.0                                               0.0               0.0             0.0             0.0                  0.0          0.0             0.0           0.0            0.0              0.0            0.0            0.0                  0.0  \n",
      "109996                           0.0                           0.0              0.0                0.0                    0.0                   0.0             0.0                0.0              0.0              0.0           0.0          0.0              0.0                0.0                 0.0                                               0.0               0.0             0.0             0.0                  0.0          0.0             0.0           0.0            0.0              0.0            0.0            0.0                  0.0  \n",
      "109997                           0.0                           0.0              0.0                0.0                    0.0                   0.0             0.0                0.0              0.0              0.0           0.0          0.0              0.0                0.0                 0.0                                               0.0               0.0             0.0             0.0                  0.0          0.0             0.0           0.0            0.0              0.0            0.0            0.0                  0.0  \n",
      "109998                           0.0                           0.0              0.0                0.0                    0.0                   0.0             0.0                0.0              0.0              0.0           0.0          0.0              0.0                0.0                 0.0                                               0.0               0.0             0.0             0.0                  0.0          0.0             0.0           0.0            0.0              0.0            0.0            0.0                  0.0  \n",
      "109999                           0.0                           0.0              0.0                0.0                    0.0                   0.0             0.0                0.0              0.0              0.0           0.0          0.0              0.0                0.0                 0.0                                               0.0               0.0             0.0             0.0                  0.0          0.0             0.0           0.0            0.0              0.0            0.0            0.0                  0.0  \n",
      "\n",
      "[110000 rows x 2805 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "total = 0\n",
    "\n",
    "chunksize = 10000\n",
    "one_hot_encoded_data = pd.DataFrame()\n",
    "\n",
    "for chunk in df:\n",
    "\n",
    "    # Drop the 'vin' column from the chunk\n",
    "    chunk.drop('vin', axis=1, inplace=True)\n",
    "\n",
    "    # Convert the 'saledate' column to datetime with timezone\n",
    "    chunk['saledate'] = pd.to_datetime(chunk['saledate'], utc=True, errors='coerce')\n",
    "\n",
    "    # Remove the timezone information\n",
    "    chunk['saledate'] = chunk['saledate'].dt.tz_localize(None)\n",
    "    \n",
    "    # Calculate the time since the sale using timezone-naive current datetime\n",
    "    chunk['days_since_sale'] = pd.Timestamp.now(tz=None) - chunk['saledate']\n",
    "\n",
    "    # Convert the timedelta to days\n",
    "    chunk['days_since_sale'] = chunk['days_since_sale'].dt.days\n",
    "\n",
    "    # Drop the 'saledate' column\n",
    "    chunk.drop('saledate', axis=1, inplace=True)\n",
    "    chunk.drop('seller', axis=1, inplace=True)\n",
    "\n",
    "    # Continue with the rest of the processing\n",
    "    \n",
    "    chunk = normalize(chunk, numeric_column)\n",
    "    chunk = one_hot_encode(chunk, cat_columns)\n",
    "    \n",
    "    chunk.fillna(0.0, inplace=True)\n",
    "    \n",
    "    one_hot_encoded_data = pd.concat([one_hot_encoded_data, chunk], ignore_index=True)\n",
    "    \n",
    "    # print dimensions of the one-hot encoded data\n",
    "    \n",
    "    total += len(chunk)\n",
    "    \n",
    "    if total > 100000:\n",
    "        break\n",
    "\n",
    "one_hot_encoded_data.fillna(0.0, inplace=True)\n",
    "print(one_hot_encoded_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = one_hot_encoded_data.sample(frac=0.8, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data = one_hot_encoded_data.drop(training_data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([88000, 2804])\n",
      "torch.Size([88000])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "tensor_x_training = torch.tensor(training_data.drop(columns=['sellingprice'], axis=1).values, dtype=torch.float32)\n",
    "tensor_y_training = torch.tensor(training_data['sellingprice'].values, dtype=torch.float32)\n",
    "\n",
    "print(tensor_x_training.shape)\n",
    "print(tensor_y_training.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "tensor_x_validation = torch.tensor(validation_data.drop(columns=['sellingprice'], axis=1).values, dtype=torch.float32)\n",
    "tensor_y_validation = torch.tensor(validation_data['sellingprice'].values, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'r2': -1, 'parameter': 0, 'lr': 0, 'patience': 0, 'ep': 0}\n",
      "{'r2': 0.0005028620638130032, 'parameter': 32, 'lr': 0.0001, 'patience': 0, 'ep': 100}\n",
      "{'r2': 0.16000962070408276, 'parameter': 32, 'lr': 0.0001, 'patience': 0, 'ep': 200}\n",
      "{'r2': 0.16000962070408276, 'parameter': 32, 'lr': 0.0001, 'patience': 0, 'ep': 200}\n",
      "{'r2': 0.16000962070408276, 'parameter': 32, 'lr': 0.0001, 'patience': 0, 'ep': 200}\n",
      "{'r2': 0.16000962070408276, 'parameter': 32, 'lr': 0.0001, 'patience': 0, 'ep': 200}\n",
      "{'r2': 0.22853186362240463, 'parameter': 32, 'lr': 0.0001, 'patience': 1, 'ep': 300}\n",
      "{'r2': 0.22853186362240463, 'parameter': 32, 'lr': 0.0001, 'patience': 1, 'ep': 300}\n",
      "{'r2': 0.22853186362240463, 'parameter': 32, 'lr': 0.0001, 'patience': 1, 'ep': 300}\n",
      "{'r2': 0.22853186362240463, 'parameter': 32, 'lr': 0.0001, 'patience': 1, 'ep': 300}\n",
      "{'r2': 0.22853186362240463, 'parameter': 32, 'lr': 0.0001, 'patience': 1, 'ep': 300}\n",
      "{'r2': 0.22853186362240463, 'parameter': 32, 'lr': 0.0001, 'patience': 1, 'ep': 300}\n",
      "{'r2': 0.22853186362240463, 'parameter': 32, 'lr': 0.0001, 'patience': 1, 'ep': 300}\n",
      "{'r2': 0.22853186362240463, 'parameter': 32, 'lr': 0.0001, 'patience': 1, 'ep': 300}\n",
      "{'r2': 0.22853186362240463, 'parameter': 32, 'lr': 0.0001, 'patience': 1, 'ep': 300}\n",
      "{'r2': 0.22853186362240463, 'parameter': 32, 'lr': 0.0001, 'patience': 1, 'ep': 300}\n",
      "{'r2': 0.22853186362240463, 'parameter': 32, 'lr': 0.0001, 'patience': 1, 'ep': 300}\n",
      "{'r2': 0.22853186362240463, 'parameter': 32, 'lr': 0.0001, 'patience': 1, 'ep': 300}\n",
      "{'r2': 0.22853186362240463, 'parameter': 32, 'lr': 0.0001, 'patience': 1, 'ep': 300}\n",
      "{'r2': 0.22853186362240463, 'parameter': 32, 'lr': 0.0001, 'patience': 1, 'ep': 300}\n",
      "{'r2': 0.22853186362240463, 'parameter': 32, 'lr': 0.0001, 'patience': 1, 'ep': 300}\n",
      "{'r2': 0.22853186362240463, 'parameter': 32, 'lr': 0.0001, 'patience': 1, 'ep': 300}\n",
      "{'r2': 0.22853186362240463, 'parameter': 32, 'lr': 0.0001, 'patience': 1, 'ep': 300}\n",
      "{'r2': 0.22853186362240463, 'parameter': 32, 'lr': 0.0001, 'patience': 1, 'ep': 300}\n",
      "{'r2': 0.22853186362240463, 'parameter': 32, 'lr': 0.0001, 'patience': 1, 'ep': 300}\n",
      "{'r2': 0.22853186362240463, 'parameter': 32, 'lr': 0.0001, 'patience': 1, 'ep': 300}\n",
      "{'r2': 0.22853186362240463, 'parameter': 32, 'lr': 0.0001, 'patience': 1, 'ep': 300}\n",
      "{'r2': 0.22853186362240463, 'parameter': 32, 'lr': 0.0001, 'patience': 1, 'ep': 300}\n",
      "{'r2': 0.22853186362240463, 'parameter': 32, 'lr': 0.0001, 'patience': 1, 'ep': 300}\n",
      "{'r2': 0.3750918919019939, 'parameter': 64, 'lr': 0.0001, 'patience': 0, 'ep': 200}\n",
      "{'r2': 0.8372288337703874, 'parameter': 64, 'lr': 0.0001, 'patience': 0, 'ep': 300}\n",
      "{'r2': 0.8372288337703874, 'parameter': 64, 'lr': 0.0001, 'patience': 0, 'ep': 300}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 64\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(ep):  \u001b[39m# maximum number of epochs\u001b[39;00m\n\u001b[1;32m     63\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 64\u001b[0m     outputs \u001b[39m=\u001b[39m model(tensor_x_training)\n\u001b[1;32m     65\u001b[0m     loss \u001b[39m=\u001b[39m criterion(outputs, tensor_y_training\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m))\n\u001b[1;32m     66\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "from torchsummary import summary\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "\n",
    "parameters = [32, 64, 128]\n",
    "learning_rate = [0.0001, 0.001, 0.01]\n",
    "patiences = [0, 1, 2]\n",
    "epochs = [100, 200, 300]\n",
    "r2_min = {\"r2\": -1, \"parameter\": 0, \"lr\": 0, \"patience\": 0, \"ep\": 0}\n",
    "\n",
    "for parameter in parameters:\n",
    "    for lr in learning_rate:\n",
    "        for patience in patiences:\n",
    "            for ep in epochs:\n",
    "                \n",
    "                print(r2_min)\n",
    "                # sequential layer\n",
    "                model = nn.Sequential(\n",
    "\n",
    "                    nn.Linear(2804, parameter),\n",
    "\n",
    "                    nn.ReLU(),\n",
    "                    \n",
    "                    nn.Linear(parameter, parameter),\n",
    "                    \n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(parameter, parameter),\n",
    "                    \n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(parameter, parameter),\n",
    "                    \n",
    "                    nn.ReLU(),\n",
    "\n",
    "                    nn.Linear(parameter, parameter),\n",
    "                    \n",
    "                    nn.ReLU(),\n",
    "\n",
    "                    nn.Linear(parameter, parameter),\n",
    "                    \n",
    "                    nn.ReLU(),\n",
    "\n",
    "                    nn.Linear(parameter, parameter),\n",
    "                    \n",
    "                    nn.ReLU(),\n",
    "\n",
    "\n",
    "\n",
    "                    nn.Linear(parameter, 1),\n",
    "\n",
    "                )\n",
    "\n",
    "                criterion = nn.MSELoss()\n",
    "\n",
    "                optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "                best_validation_loss = float('inf')\n",
    "                counter = 0\n",
    "\n",
    "                for epoch in range(ep):  # maximum number of epochs\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(tensor_x_training)\n",
    "                    loss = criterion(outputs, tensor_y_training.unsqueeze(1))\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    # Calculate validation loss\n",
    "                    with torch.no_grad():\n",
    "                        validation_outputs = model(tensor_x_validation)\n",
    "                        validation_loss = criterion(validation_outputs, tensor_y_validation.unsqueeze(1))\n",
    "\n",
    "                    # Check for early stopping\n",
    "                    if validation_loss < best_validation_loss:\n",
    "                        best_validation_loss = validation_loss\n",
    "                        counter = 0\n",
    "                    else:\n",
    "                        counter += 1\n",
    "                        if counter >= patience:\n",
    "                            break\n",
    "\n",
    "                #plt.plot(losses)\n",
    "\n",
    "                from sklearn.metrics import r2_score\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    validation_outputs = model(tensor_x_validation)\n",
    "                    r2 = r2_score(tensor_y_validation.numpy(), validation_outputs.numpy())\n",
    "\n",
    "                    if r2 > r2_min['r2']:\n",
    "                        r2_min['r2'] = r2\n",
    "                        r2_min['parameter'] = parameter\n",
    "                        r2_min['lr'] = lr\n",
    "                        r2_min['patience'] = patience\n",
    "                        r2_min['ep'] = ep\n",
    "\n",
    "                #print(f\"R^2 Score: {r2}\")\n",
    "        \n",
    "print(\"stopping\")\n",
    "print(r2_min)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {'r2': -1, 'parameter': 0, 'lr': 0, 'patience': 0, 'ep': 0}\n",
    "# {'r2': 0.0005028620638130032, 'parameter': 32, 'lr': 0.0001, 'patience': 0, 'ep': 100}\n",
    "# {'r2': 0.16000962070408276, 'parameter': 32, 'lr': 0.0001, 'patience': 0, 'ep': 200}\n",
    "# {'r2': 0.16000962070408276, 'parameter': 32, 'lr': 0.0001, 'patience': 0, 'ep': 200}\n",
    "# {'r2': 0.16000962070408276, 'parameter': 32, 'lr': 0.0001, 'patience': 0, 'ep': 200}\n",
    "# {'r2': 0.16000962070408276, 'parameter': 32, 'lr': 0.0001, 'patience': 0, 'ep': 200}\n",
    "# {'r2': 0.22853186362240463, 'parameter': 32, 'lr': 0.0001, 'patience': 1, 'ep': 300}\n",
    "# {'r2': 0.22853186362240463, 'parameter': 32, 'lr': 0.0001, 'patience': 1, 'ep': 300}\n",
    "# {'r2': 0.22853186362240463, 'parameter': 32, 'lr': 0.0001, 'patience': 1, 'ep': 300}\n",
    "# {'r2': 0.22853186362240463, 'parameter': 32, 'lr': 0.0001, 'patience': 1, 'ep': 300}\n",
    "# {'r2': 0.22853186362240463, 'parameter': 32, 'lr': 0.0001, 'patience': 1, 'ep': 300}\n",
    "# {'r2': 0.22853186362240463, 'parameter': 32, 'lr': 0.0001, 'patience': 1, 'ep': 300}\n",
    "# {'r2': 0.22853186362240463, 'parameter': 32, 'lr': 0.0001, 'patience': 1, 'ep': 300}\n",
    "# {'r2': 0.22853186362240463, 'parameter': 32, 'lr': 0.0001, 'patience': 1, 'ep': 300}\n",
    "# {'r2': 0.22853186362240463, 'parameter': 32, 'lr': 0.0001, 'patience': 1, 'ep': 300}\n",
    "# {'r2': 0.22853186362240463, 'parameter': 32, 'lr': 0.0001, 'patience': 1, 'ep': 300}\n",
    "# {'r2': 0.22853186362240463, 'parameter': 32, 'lr': 0.0001, 'patience': 1, 'ep': 300}\n",
    "# {'r2': 0.22853186362240463, 'parameter': 32, 'lr': 0.0001, 'patience': 1, 'ep': 300}\n",
    "# {'r2': 0.22853186362240463, 'parameter': 32, 'lr': 0.0001, 'patience': 1, 'ep': 300}\n",
    "# {'r2': 0.22853186362240463, 'parameter': 32, 'lr': 0.0001, 'patience': 1, 'ep': 300}\n",
    "# {'r2': 0.22853186362240463, 'parameter': 32, 'lr': 0.0001, 'patience': 1, 'ep': 300}\n",
    "# {'r2': 0.22853186362240463, 'parameter': 32, 'lr': 0.0001, 'patience': 1, 'ep': 300}\n",
    "# {'r2': 0.22853186362240463, 'parameter': 32, 'lr': 0.0001, 'patience': 1, 'ep': 300}\n",
    "# {'r2': 0.22853186362240463, 'parameter': 32, 'lr': 0.0001, 'patience': 1, 'ep': 300}\n",
    "# {'r2': 0.22853186362240463, 'parameter': 32, 'lr': 0.0001, 'patience': 1, 'ep': 300}\n",
    "# ...\n",
    "# {'r2': 0.22853186362240463, 'parameter': 32, 'lr': 0.0001, 'patience': 1, 'ep': 300}\n",
    "# {'r2': 0.3750918919019939, 'parameter': 64, 'lr': 0.0001, 'patience': 0, 'ep': 200}\n",
    "# {'r2': 0.8372288337703874, 'parameter': 64, 'lr': 0.0001, 'patience': 0, 'ep': 300}\n",
    "# {'r2': 0.8372288337703874, 'parameter': 64, 'lr': 0.0001, 'patience': 0, 'ep': 300}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
