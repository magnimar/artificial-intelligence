{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"car_prices.csv\", chunksize=10000)\n",
    "\n",
    "cat_columns = [\n",
    "    'make', \n",
    "    'model', \n",
    "    'trim', \n",
    "    'body', \n",
    "    'transmission', \n",
    "    'state', \n",
    "    'color', \n",
    "    'interior', \n",
    "]\n",
    "\n",
    "numeric_column = [\n",
    "    'year', \n",
    "    'condition', \n",
    "    'odometer', \n",
    "    'mmr', \n",
    "    'sellingprice',\n",
    "    'days_since_sale'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(df, columns):\n",
    "    \"\"\"\n",
    "    One-hot encodes specified columns in a pandas DataFrame.\n",
    "    \"\"\"\n",
    "    for column in columns:\n",
    "        dummies = pd.get_dummies(df[column], prefix=column)\n",
    "        df = pd.concat([df, dummies], axis=1)\n",
    "        df.drop(column, axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "def normalize(df, columns):\n",
    "    \"\"\"\n",
    "    Normalizes specified columns in a pandas DataFrame between 0 and 1.\n",
    "    \"\"\"\n",
    "    for column in columns:\n",
    "        df[column] = (df[column] - df[column].min()) / (df[column].max() - df[column].min())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            year  condition  odometer       mmr  sellingprice  days_since_sale  make_BMW  make_Chevrolet  make_Chrysler  make_Dodge  make_Ford  make_Honda  make_Hyundai  make_Infiniti  make_Kia  make_Mazda  make_Mercedes-Benz  make_Nissan  make_Toyota  make_Volkswagen  make_other  model_1500  model_200  model_2500  model_3 Series  model_300  model_4Runner  model_5 Series  model_6 Series  model_7 Series  model_A4  model_A6  model_Acadia  model_Accent  model_Accord  model_Altima  model_Avalanche  model_Avalon  model_Avenger  model_C-Class  model_CC  model_CLK-Class  model_CR-V  model_CTS  model_CX-5  model_CX-9  model_Caliber  model_Camaro  model_Camry  model_Challenger  ...  model_Juke  model_MKS  trim_E350 Sport  trim_Two  model_Savana Cargo  trim_EL King Ranch  trim_GLK350  trim_GT Premium  state_wa  trim_SLE-1  model_Outlander Sport  trim_TDI  model_Versa Note  trim_King Ranch  trim_LX Hybrid  trim_PreRunner V6  trim_SR  model_Mazda5  trim_LTZ 1500  trim_E350 Luxury 4MATIC  \\\n",
      "0       1.000000      1.000  0.016638  0.137439      0.139605         0.371115       0.0               0            0.0           0          0         0.0           0.0            0.0       1.0         0.0                 0.0            0            0              0.0           0         0.0        0.0         0.0               0          0            0.0             0.0             0.0             0.0       0.0       0.0           0.0           0.0             0             0              0.0           0.0            0.0              0       0.0              0.0           0        0.0         0.0         0.0            0.0           0.0            0               0.0  ...         0.0        0.0              0.0       0.0                 0.0                 0.0          0.0              0.0       0.0         0.0                    0.0       0.0               0.0              0.0             0.0                0.0      0.0           0.0            0.0                      0.0   \n",
      "1       1.000000      1.000  0.009392  0.139453      0.139605         0.371115       0.0               0            0.0           0          0         0.0           0.0            0.0       1.0         0.0                 0.0            0            0              0.0           0         0.0        0.0         0.0               0          0            0.0             0.0             0.0             0.0       0.0       0.0           0.0           0.0             0             0              0.0           0.0            0.0              0       0.0              0.0           0        0.0         0.0         0.0            0.0           0.0            0               0.0  ...         0.0        0.0              0.0       0.0                 0.0                 0.0          0.0              0.0       0.0         0.0                    0.0       0.0               0.0              0.0             0.0                0.0      0.0           0.0            0.0                      0.0   \n",
      "2       0.965517      0.875  0.001330  0.213962      0.194800         0.318099       1.0               0            0.0           0          0         0.0           0.0            0.0       0.0         0.0                 0.0            0            0              0.0           0         0.0        0.0         0.0               1          0            0.0             0.0             0.0             0.0       0.0       0.0           0.0           0.0             0             0              0.0           0.0            0.0              0       0.0              0.0           0        0.0         0.0         0.0            0.0           0.0            0               0.0  ...         0.0        0.0              0.0       0.0                 0.0                 0.0          0.0              0.0       0.0         0.0                    0.0       0.0               0.0              0.0             0.0                0.0      0.0           0.0            0.0                      0.0   \n",
      "3       1.000000      0.775  0.014281  0.184427      0.180189         0.292505       0.0               0            0.0           0          0         0.0           0.0            0.0       0.0         0.0                 0.0            0            0              0.0           1         0.0        0.0         0.0               0          0            0.0             0.0             0.0             0.0       0.0       0.0           0.0           0.0             0             0              0.0           0.0            0.0              0       0.0              0.0           0        0.0         0.0         0.0            0.0           0.0            0               0.0  ...         0.0        0.0              0.0       0.0                 0.0                 0.0          0.0              0.0       0.0         0.0                    0.0       0.0               0.0              0.0             0.0                0.0      0.0           0.0            0.0                      0.0   \n",
      "4       0.965517      0.825  0.002640  0.442860      0.435061         0.367459       1.0               0            0.0           0          0         0.0           0.0            0.0       0.0         0.0                 0.0            0            0              0.0           0         0.0        0.0         0.0               0          0            0.0             0.0             0.0             0.0       0.0       0.0           0.0           0.0             0             0              0.0           0.0            0.0              0       0.0              0.0           0        0.0         0.0         0.0            0.0           0.0            0               0.0  ...         0.0        0.0              0.0       0.0                 0.0                 0.0          0.0              0.0       0.0         0.0                    0.0       0.0               0.0              0.0             0.0                0.0      0.0           0.0            0.0                      0.0   \n",
      "...          ...        ...       ...       ...           ...              ...       ...             ...            ...         ...        ...         ...           ...            ...       ...         ...                 ...          ...          ...              ...         ...         ...        ...         ...             ...        ...            ...             ...             ...             ...       ...       ...           ...           ...           ...           ...              ...           ...            ...            ...       ...              ...         ...        ...         ...         ...            ...           ...          ...               ...  ...         ...        ...              ...       ...                 ...                 ...          ...              ...       ...         ...                    ...       ...               ...              ...             ...                ...      ...           ...            ...                      ...   \n",
      "433993  0.904762      0.800  0.029170  0.151252      0.161994         0.819672       0.0               0            0.0           0          0         0.0           0.0            0.0       0.0         0.0                 0.0            1            0              0.0           0         0.0        0.0         0.0               0          0            0.0             0.0             0.0             0.0       0.0       0.0           0.0           0.0             0             0              0.0           0.0            0.0              0       0.0              0.0           0        0.0         0.0         0.0            0.0           0.0            0               0.0  ...         0.0        0.0              0.0       0.0                 0.0                 0.0          0.0              0.0       0.0         0.0                    0.0       0.0               0.0              0.0             0.0                0.0      0.0           0.0            0.0                      0.0   \n",
      "433994  0.857143      0.675  0.131005  0.235565      0.265836         0.819672       0.0               0            0.0           0          0         0.0           0.0            0.0       0.0         0.0                 0.0            0            0              0.0           1         0.0        0.0         0.0               0          0            0.0             0.0             0.0             0.0       0.0       0.0           0.0           0.0             0             0              0.0           0.0            0.0              0       0.0              0.0           0        0.0         0.0         0.0            0.0           0.0            0               0.0  ...         0.0        0.0              0.0       0.0                 0.0                 0.0          0.0              0.0       0.0         0.0                    0.0       0.0               0.0              0.0             0.0                0.0      0.0           0.0            0.0                      0.0   \n",
      "433995  0.857143      0.900  0.222012  0.083035      0.083074         0.901639       0.0               1            0.0           0          0         0.0           0.0            0.0       0.0         0.0                 0.0            0            0              0.0           0         0.0        0.0         0.0               0          0            0.0             0.0             0.0             0.0       0.0       0.0           0.0           0.0             0             0              0.0           0.0            0.0              0       0.0              0.0           0        0.0         0.0         0.0            0.0           0.0            0               0.0  ...         0.0        0.0              0.0       0.0                 0.0                 0.0          0.0              0.0       0.0         0.0                    0.0       0.0               0.0              0.0             0.0                0.0      0.0           0.0            0.0                      0.0   \n",
      "433996  0.857143      0.400  0.238272  0.228411      0.229491         0.606557       0.0               1            0.0           0          0         0.0           0.0            0.0       0.0         0.0                 0.0            0            0              0.0           0         0.0        0.0         0.0               0          0            0.0             0.0             0.0             0.0       0.0       0.0           0.0           0.0             0             0              0.0           0.0            0.0              0       0.0              0.0           0        0.0         0.0         0.0            0.0           0.0            0               0.0  ...         0.0        0.0              0.0       0.0                 0.0                 0.0          0.0              0.0       0.0         0.0                    0.0       0.0               0.0              0.0             0.0                0.0      0.0           0.0            0.0                      0.0   \n",
      "433997  0.857143      0.850  0.091636  0.113439      0.119418         0.819672       0.0               1            0.0           0          0         0.0           0.0            0.0       0.0         0.0                 0.0            0            0              0.0           0         0.0        0.0         0.0               0          0            0.0             0.0             0.0             0.0       0.0       0.0           0.0           0.0             0             0              0.0           0.0            0.0              0       0.0              0.0           0        0.0         0.0         0.0            0.0           0.0            0               0.0  ...         0.0        0.0              0.0       0.0                 0.0                 0.0          0.0              0.0       0.0         0.0                    0.0       0.0               0.0              0.0             0.0                0.0      0.0           0.0            0.0                      0.0   \n",
      "\n",
      "        model_Cooper Countryman  trim_E-350 Super Duty  body_Crew Cab  trim_Pop  model_CT 200h  model_QX80  trim_STX  model_XC60  trim_2.0 SR  trim_T5 Drive-E Premier  model_Venza  model_Tiguan  trim_2SS  model_Regal  model_500  model_Corvette  model_QX  model_fortwo  trim_LS 3500  trim_QX56  trim_T5 Drive-E  trim_Trekking  trim_passion coupe  model_Santa Fe Sport  state_co  color_red  model_JX  trim_JX35  body_G Sedan  state_sc  \n",
      "0                           0.0                    0.0            0.0       0.0            0.0         0.0       0.0         0.0          0.0                      0.0          0.0           0.0       0.0          0.0        0.0             0.0       0.0           0.0           0.0        0.0              0.0            0.0                 0.0                   0.0       0.0        0.0       0.0        0.0           0.0       0.0  \n",
      "1                           0.0                    0.0            0.0       0.0            0.0         0.0       0.0         0.0          0.0                      0.0          0.0           0.0       0.0          0.0        0.0             0.0       0.0           0.0           0.0        0.0              0.0            0.0                 0.0                   0.0       0.0        0.0       0.0        0.0           0.0       0.0  \n",
      "2                           0.0                    0.0            0.0       0.0            0.0         0.0       0.0         0.0          0.0                      0.0          0.0           0.0       0.0          0.0        0.0             0.0       0.0           0.0           0.0        0.0              0.0            0.0                 0.0                   0.0       0.0        0.0       0.0        0.0           0.0       0.0  \n",
      "3                           0.0                    0.0            0.0       0.0            0.0         0.0       0.0         0.0          0.0                      0.0          0.0           0.0       0.0          0.0        0.0             0.0       0.0           0.0           0.0        0.0              0.0            0.0                 0.0                   0.0       0.0        0.0       0.0        0.0           0.0       0.0  \n",
      "4                           0.0                    0.0            0.0       0.0            0.0         0.0       0.0         0.0          0.0                      0.0          0.0           0.0       0.0          0.0        0.0             0.0       0.0           0.0           0.0        0.0              0.0            0.0                 0.0                   0.0       0.0        0.0       0.0        0.0           0.0       0.0  \n",
      "...                         ...                    ...            ...       ...            ...         ...       ...         ...          ...                      ...          ...           ...       ...          ...        ...             ...       ...           ...           ...        ...              ...            ...                 ...                   ...       ...        ...       ...        ...           ...       ...  \n",
      "433993                      0.0                    0.0            0.0       0.0            0.0         0.0       0.0         0.0          0.0                      0.0          0.0           0.0       0.0          0.0        0.0             0.0       0.0           0.0           0.0        0.0              0.0            0.0                 0.0                   0.0       0.0        0.0       0.0        0.0           0.0       0.0  \n",
      "433994                      0.0                    0.0            0.0       0.0            0.0         0.0       0.0         0.0          0.0                      0.0          0.0           0.0       0.0          0.0        0.0             0.0       0.0           0.0           0.0        0.0              0.0            0.0                 0.0                   0.0       0.0        0.0       0.0        0.0           0.0       0.0  \n",
      "433995                      0.0                    0.0            0.0       0.0            0.0         0.0       0.0         0.0          0.0                      0.0          0.0           0.0       0.0          0.0        0.0             0.0       0.0           0.0           0.0        0.0              0.0            0.0                 0.0                   0.0       0.0        0.0       0.0        0.0           0.0       0.0  \n",
      "433996                      0.0                    0.0            0.0       0.0            0.0         0.0       0.0         0.0          0.0                      0.0          0.0           0.0       0.0          0.0        0.0             0.0       0.0           0.0           0.0        0.0              0.0            0.0                 0.0                   0.0       0.0        0.0       0.0        0.0           0.0       0.0  \n",
      "433997                      0.0                    0.0            0.0       0.0            0.0         0.0       0.0         0.0          0.0                      0.0          0.0           0.0       0.0          0.0        0.0             0.0       0.0           0.0           0.0        0.0              0.0            0.0                 0.0                   0.0       0.0        0.0       0.0        0.0           0.0       0.0  \n",
      "\n",
      "[433998 rows x 600 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "chunksize = 10000\n",
    "one_hot_encoded_data = pd.DataFrame()\n",
    "\n",
    "for chunk in df:\n",
    "\n",
    "    # Drop the 'vin' column from the chunk\n",
    "    chunk.drop('vin', axis=1, inplace=True)\n",
    "\n",
    "    # Convert the 'saledate' column to datetime with timezone\n",
    "    chunk['saledate'] = pd.to_datetime(chunk['saledate'], utc=True, errors='coerce')\n",
    "\n",
    "    # Remove the timezone information\n",
    "    chunk['saledate'] = chunk['saledate'].dt.tz_localize(None)\n",
    "    \n",
    "    # Calculate the time since the sale using timezone-naive current datetime\n",
    "    chunk['days_since_sale'] = pd.Timestamp.now(tz=None) - chunk['saledate']\n",
    "\n",
    "    # Convert the timedelta to days\n",
    "    chunk['days_since_sale'] = chunk['days_since_sale'].dt.days\n",
    "\n",
    "    # Drop the 'saledate' column\n",
    "    chunk.drop('saledate', axis=1, inplace=True)\n",
    "    chunk.drop('seller', axis=1, inplace=True)\n",
    "\n",
    "    # Continue with the rest of the processing\n",
    "    \n",
    "    # get sum of all values in each column\n",
    "    \n",
    "    for column in cat_columns:\n",
    "    \n",
    "        make_frequency = chunk[column].value_counts().sort_values(ascending=False)\n",
    "\n",
    "        # Calculate the cumulative frequency distribution\n",
    "        cumulative_frequency = make_frequency.cumsum() / make_frequency.sum()\n",
    "\n",
    "        # Select categories in the top 80% of the frequency distribution\n",
    "        selected_categories = cumulative_frequency[cumulative_frequency <= 0.8].index\n",
    "\n",
    "        # Replace the other categories in the 'make' column with \"other\"\n",
    "        chunk[column] = chunk[column].apply(lambda x: x if x in selected_categories else 'other')\n",
    "    \n",
    "    chunk = normalize(chunk, numeric_column)\n",
    "    chunk = one_hot_encode(chunk, cat_columns)\n",
    "    \n",
    "    chunk.fillna(0.0, inplace=True)\n",
    "    \n",
    "    one_hot_encoded_data = pd.concat([one_hot_encoded_data, chunk], ignore_index=True)\n",
    "    \n",
    "    # print dimensions of the one-hot encoded data\n",
    "    \n",
    "one_hot_encoded_data.fillna(0.0, inplace=True)\n",
    "print(one_hot_encoded_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = one_hot_encoded_data.sample(frac=0.8, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data = one_hot_encoded_data.drop(training_data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "599\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "tensor_x_training = torch.tensor(training_data.drop(columns=['sellingprice'], axis=1).values, dtype=torch.float32)\n",
    "tensor_y_training = torch.tensor(training_data['sellingprice'].values, dtype=torch.float32)\n",
    "\n",
    "print(tensor_x_training.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "tensor_x_validation = torch.tensor(validation_data.drop(columns=['sellingprice'], axis=1).values, dtype=torch.float32)\n",
    "tensor_y_validation = torch.tensor(validation_data['sellingprice'].values, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing out parameter:  8  and learning rate:  0.0001\n",
      "current r2 min :  {'r2': -1, 'parameter': 0, 'learning_rate': 0}\n",
      "r2:  -0.6861637323364125\n",
      "best validation loss:  tensor(0.0122)  validation loss:  tensor(0.0122)\n",
      "r2:  -0.6752192951288296\n",
      "best validation loss:  tensor(0.0121)  validation loss:  tensor(0.0121)\n",
      "r2:  -0.6643767327905838\n",
      "best validation loss:  tensor(0.0121)  validation loss:  tensor(0.0121)\n",
      "r2:  -0.6536352317422531\n",
      "best validation loss:  tensor(0.0120)  validation loss:  tensor(0.0120)\n",
      "r2:  -0.6429921824048177\n",
      "best validation loss:  tensor(0.0119)  validation loss:  tensor(0.0119)\n",
      "r2:  -0.632452280170118\n",
      "best validation loss:  tensor(0.0118)  validation loss:  tensor(0.0118)\n",
      "r2:  -0.6220160216388269\n",
      "best validation loss:  tensor(0.0118)  validation loss:  tensor(0.0118)\n",
      "r2:  -0.6116844712065697\n",
      "best validation loss:  tensor(0.0117)  validation loss:  tensor(0.0117)\n",
      "r2:  -0.6014604881299173\n",
      "best validation loss:  tensor(0.0116)  validation loss:  tensor(0.0116)\n",
      "r2:  -0.5913427614501527\n",
      "best validation loss:  tensor(0.0115)  validation loss:  tensor(0.0115)\n",
      "r2:  -0.5813332496783836\n",
      "best validation loss:  tensor(0.0115)  validation loss:  tensor(0.0115)\n",
      "r2:  -0.5714336204607882\n",
      "best validation loss:  tensor(0.0114)  validation loss:  tensor(0.0114)\n",
      "r2:  -0.5616432821413033\n",
      "best validation loss:  tensor(0.0113)  validation loss:  tensor(0.0113)\n",
      "r2:  -0.551962728872162\n",
      "best validation loss:  tensor(0.0113)  validation loss:  tensor(0.0113)\n",
      "r2:  -0.5423919443706255\n",
      "best validation loss:  tensor(0.0112)  validation loss:  tensor(0.0112)\n",
      "r2:  -0.5329294922827326\n",
      "best validation loss:  tensor(0.0111)  validation loss:  tensor(0.0111)\n",
      "r2:  -0.5235744837660259\n",
      "best validation loss:  tensor(0.0110)  validation loss:  tensor(0.0110)\n",
      "r2:  -0.5143262204927479\n",
      "best validation loss:  tensor(0.0110)  validation loss:  tensor(0.0110)\n",
      "r2:  -0.5051866703232946\n",
      "best validation loss:  tensor(0.0109)  validation loss:  tensor(0.0109)\n",
      "r2:  -0.49615718385103613\n",
      "best validation loss:  tensor(0.0108)  validation loss:  tensor(0.0108)\n",
      "r2:  -0.48723525926902855\n",
      "best validation loss:  tensor(0.0108)  validation loss:  tensor(0.0108)\n",
      "r2:  -0.4784225094161132\n",
      "best validation loss:  tensor(0.0107)  validation loss:  tensor(0.0107)\n",
      "r2:  -0.4697183644134484\n",
      "best validation loss:  tensor(0.0107)  validation loss:  tensor(0.0107)\n",
      "r2:  -0.4611216794316315\n",
      "best validation loss:  tensor(0.0106)  validation loss:  tensor(0.0106)\n",
      "r2:  -0.4526325063406691\n",
      "best validation loss:  tensor(0.0105)  validation loss:  tensor(0.0105)\n",
      "r2:  -0.44425255949574116\n",
      "best validation loss:  tensor(0.0105)  validation loss:  tensor(0.0105)\n",
      "r2:  -0.4359786511223447\n",
      "best validation loss:  tensor(0.0104)  validation loss:  tensor(0.0104)\n",
      "r2:  -0.4278096866913259\n",
      "best validation loss:  tensor(0.0104)  validation loss:  tensor(0.0104)\n",
      "r2:  -0.4197446225274948\n",
      "best validation loss:  tensor(0.0103)  validation loss:  tensor(0.0103)\n",
      "r2:  -0.41178126213876287\n",
      "best validation loss:  tensor(0.0102)  validation loss:  tensor(0.0102)\n",
      "r2:  -0.40391775445055256\n",
      "best validation loss:  tensor(0.0102)  validation loss:  tensor(0.0102)\n",
      "r2:  -0.39615495383930055\n",
      "best validation loss:  tensor(0.0101)  validation loss:  tensor(0.0101)\n",
      "r2:  -0.388492908597166\n",
      "best validation loss:  tensor(0.0101)  validation loss:  tensor(0.0101)\n",
      "r2:  -0.3809303928608798\n",
      "best validation loss:  tensor(0.0100)  validation loss:  tensor(0.0100)\n",
      "r2:  -0.3734681263011683\n",
      "best validation loss:  tensor(0.0100)  validation loss:  tensor(0.0100)\n",
      "r2:  -0.3661071412681489\n",
      "best validation loss:  tensor(0.0099)  validation loss:  tensor(0.0099)\n",
      "r2:  -0.35884544964010145\n",
      "best validation loss:  tensor(0.0099)  validation loss:  tensor(0.0099)\n",
      "r2:  -0.351682456704987\n",
      "best validation loss:  tensor(0.0098)  validation loss:  tensor(0.0098)\n",
      "r2:  -0.3446161667026275\n",
      "best validation loss:  tensor(0.0097)  validation loss:  tensor(0.0097)\n",
      "r2:  -0.33764592814240446\n",
      "best validation loss:  tensor(0.0097)  validation loss:  tensor(0.0097)\n",
      "r2:  -0.3307704998409582\n",
      "best validation loss:  tensor(0.0096)  validation loss:  tensor(0.0096)\n",
      "r2:  -0.3239883848764338\n",
      "best validation loss:  tensor(0.0096)  validation loss:  tensor(0.0096)\n",
      "r2:  -0.3172994259473563\n",
      "best validation loss:  tensor(0.0095)  validation loss:  tensor(0.0095)\n",
      "r2:  -0.3107044160108321\n",
      "best validation loss:  tensor(0.0095)  validation loss:  tensor(0.0095)\n",
      "r2:  -0.30420155992675646\n",
      "best validation loss:  tensor(0.0095)  validation loss:  tensor(0.0095)\n",
      "r2:  -0.2977897105647769\n",
      "best validation loss:  tensor(0.0094)  validation loss:  tensor(0.0094)\n",
      "r2:  -0.2914670033310245\n",
      "best validation loss:  tensor(0.0094)  validation loss:  tensor(0.0094)\n",
      "r2:  -0.2852317537302296\n",
      "best validation loss:  tensor(0.0093)  validation loss:  tensor(0.0093)\n",
      "r2:  -0.27908377991618427\n",
      "best validation loss:  tensor(0.0093)  validation loss:  tensor(0.0093)\n",
      "r2:  -0.27302293886307183\n",
      "best validation loss:  tensor(0.0092)  validation loss:  tensor(0.0092)\n",
      "r2:  -0.26704949341246276\n",
      "best validation loss:  tensor(0.0092)  validation loss:  tensor(0.0092)\n",
      "r2:  -0.26116156098364596\n",
      "best validation loss:  tensor(0.0091)  validation loss:  tensor(0.0091)\n",
      "r2:  -0.2553584575242507\n",
      "best validation loss:  tensor(0.0091)  validation loss:  tensor(0.0091)\n",
      "r2:  -0.24963899389305322\n",
      "best validation loss:  tensor(0.0091)  validation loss:  tensor(0.0091)\n",
      "r2:  -0.2440023543824057\n",
      "best validation loss:  tensor(0.0090)  validation loss:  tensor(0.0090)\n",
      "r2:  -0.23844798461427152\n",
      "best validation loss:  tensor(0.0090)  validation loss:  tensor(0.0090)\n",
      "r2:  -0.23297415436593205\n",
      "best validation loss:  tensor(0.0089)  validation loss:  tensor(0.0089)\n",
      "r2:  -0.2275799155215208\n",
      "best validation loss:  tensor(0.0089)  validation loss:  tensor(0.0089)\n",
      "r2:  -0.2222649376664967\n",
      "best validation loss:  tensor(0.0089)  validation loss:  tensor(0.0089)\n",
      "r2:  -0.21702887733802534\n",
      "best validation loss:  tensor(0.0088)  validation loss:  tensor(0.0088)\n",
      "r2:  -0.2118715339534607\n",
      "best validation loss:  tensor(0.0088)  validation loss:  tensor(0.0088)\n",
      "r2:  -0.20679170664032687\n",
      "best validation loss:  tensor(0.0087)  validation loss:  tensor(0.0087)\n",
      "r2:  -0.20178885586142026\n",
      "best validation loss:  tensor(0.0087)  validation loss:  tensor(0.0087)\n",
      "r2:  -0.19686309360197485\n",
      "best validation loss:  tensor(0.0087)  validation loss:  tensor(0.0087)\n",
      "r2:  -0.19201269191616532\n",
      "best validation loss:  tensor(0.0086)  validation loss:  tensor(0.0086)\n",
      "r2:  -0.18723779971066\n",
      "best validation loss:  tensor(0.0086)  validation loss:  tensor(0.0086)\n",
      "r2:  -0.18253723332379224\n",
      "best validation loss:  tensor(0.0086)  validation loss:  tensor(0.0086)\n",
      "r2:  -0.17791033732143458\n",
      "best validation loss:  tensor(0.0085)  validation loss:  tensor(0.0085)\n",
      "r2:  -0.17335615880108035\n",
      "best validation loss:  tensor(0.0085)  validation loss:  tensor(0.0085)\n",
      "r2:  -0.16887469585211967\n",
      "best validation loss:  tensor(0.0085)  validation loss:  tensor(0.0085)\n",
      "r2:  -0.1644646522172586\n",
      "best validation loss:  tensor(0.0084)  validation loss:  tensor(0.0084)\n",
      "r2:  -0.16012592215471022\n",
      "best validation loss:  tensor(0.0084)  validation loss:  tensor(0.0084)\n",
      "r2:  -0.15585806104778754\n",
      "best validation loss:  tensor(0.0084)  validation loss:  tensor(0.0084)\n",
      "r2:  -0.15166015021577728\n",
      "best validation loss:  tensor(0.0083)  validation loss:  tensor(0.0083)\n",
      "r2:  -0.14753183093356603\n",
      "best validation loss:  tensor(0.0083)  validation loss:  tensor(0.0083)\n",
      "r2:  -0.14347245197254255\n",
      "best validation loss:  tensor(0.0083)  validation loss:  tensor(0.0083)\n",
      "r2:  -0.13948145750666474\n",
      "best validation loss:  tensor(0.0083)  validation loss:  tensor(0.0083)\n",
      "r2:  -0.13555755617609444\n",
      "best validation loss:  tensor(0.0082)  validation loss:  tensor(0.0082)\n",
      "r2:  -0.1316997870877361\n",
      "best validation loss:  tensor(0.0082)  validation loss:  tensor(0.0082)\n",
      "r2:  -0.12790825214439172\n",
      "best validation loss:  tensor(0.0082)  validation loss:  tensor(0.0082)\n",
      "r2:  -0.12418194722881193\n",
      "best validation loss:  tensor(0.0081)  validation loss:  tensor(0.0081)\n",
      "r2:  -0.12051982507638881\n",
      "best validation loss:  tensor(0.0081)  validation loss:  tensor(0.0081)\n",
      "r2:  -0.11692137090740085\n",
      "best validation loss:  tensor(0.0081)  validation loss:  tensor(0.0081)\n",
      "r2:  -0.1133855964813042\n",
      "best validation loss:  tensor(0.0081)  validation loss:  tensor(0.0081)\n",
      "r2:  -0.10991214163539631\n",
      "best validation loss:  tensor(0.0080)  validation loss:  tensor(0.0080)\n",
      "r2:  -0.10650065435480927\n",
      "best validation loss:  tensor(0.0080)  validation loss:  tensor(0.0080)\n",
      "r2:  -0.10315101307038854\n",
      "best validation loss:  tensor(0.0080)  validation loss:  tensor(0.0080)\n",
      "r2:  -0.09986230396831153\n",
      "best validation loss:  tensor(0.0080)  validation loss:  tensor(0.0080)\n",
      "r2:  -0.09663383085074662\n",
      "best validation loss:  tensor(0.0079)  validation loss:  tensor(0.0079)\n",
      "r2:  -0.09346423720098196\n",
      "best validation loss:  tensor(0.0079)  validation loss:  tensor(0.0079)\n",
      "r2:  -0.09035335448264359\n",
      "best validation loss:  tensor(0.0079)  validation loss:  tensor(0.0079)\n",
      "r2:  -0.08730047372835426\n",
      "best validation loss:  tensor(0.0079)  validation loss:  tensor(0.0079)\n",
      "r2:  -0.08430464762447665\n",
      "best validation loss:  tensor(0.0079)  validation loss:  tensor(0.0079)\n",
      "r2:  -0.08136516460893195\n",
      "best validation loss:  tensor(0.0078)  validation loss:  tensor(0.0078)\n",
      "r2:  -0.07848170651985087\n",
      "best validation loss:  tensor(0.0078)  validation loss:  tensor(0.0078)\n",
      "r2:  -0.07565346554921715\n",
      "best validation loss:  tensor(0.0078)  validation loss:  tensor(0.0078)\n",
      "r2:  -0.07287931364827194\n",
      "best validation loss:  tensor(0.0078)  validation loss:  tensor(0.0078)\n",
      "r2:  -0.07015853850743858\n",
      "best validation loss:  tensor(0.0078)  validation loss:  tensor(0.0078)\n",
      "r2:  -0.06749035785792823\n",
      "best validation loss:  tensor(0.0077)  validation loss:  tensor(0.0077)\n",
      "r2:  -0.0648742109844116\n",
      "best validation loss:  tensor(0.0077)  validation loss:  tensor(0.0077)\n",
      "r2:  -0.062309601191406605\n",
      "best validation loss:  tensor(0.0077)  validation loss:  tensor(0.0077)\n",
      "r2:  -0.0597953081109337\n",
      "best validation loss:  tensor(0.0077)  validation loss:  tensor(0.0077)\n",
      "r2:  -0.0573308213720336\n",
      "best validation loss:  tensor(0.0077)  validation loss:  tensor(0.0077)\n",
      "r2:  -0.05491514701740652\n",
      "best validation loss:  tensor(0.0076)  validation loss:  tensor(0.0076)\n",
      "r2:  -0.052547360986517244\n",
      "best validation loss:  tensor(0.0076)  validation loss:  tensor(0.0076)\n",
      "r2:  -0.05022687638226819\n",
      "best validation loss:  tensor(0.0076)  validation loss:  tensor(0.0076)\n",
      "r2:  -0.04795257183230128\n",
      "best validation loss:  tensor(0.0076)  validation loss:  tensor(0.0076)\n",
      "r2:  -0.04572384119601014\n",
      "best validation loss:  tensor(0.0076)  validation loss:  tensor(0.0076)\n",
      "r2:  -0.043539801221402996\n",
      "best validation loss:  tensor(0.0076)  validation loss:  tensor(0.0076)\n",
      "r2:  -0.04139973634433591\n",
      "best validation loss:  tensor(0.0075)  validation loss:  tensor(0.0075)\n",
      "r2:  -0.03930297370451297\n",
      "best validation loss:  tensor(0.0075)  validation loss:  tensor(0.0075)\n",
      "r2:  -0.037248435708449934\n",
      "best validation loss:  tensor(0.0075)  validation loss:  tensor(0.0075)\n",
      "r2:  -0.03523555779666143\n",
      "best validation loss:  tensor(0.0075)  validation loss:  tensor(0.0075)\n",
      "r2:  -0.033263150333370683\n",
      "best validation loss:  tensor(0.0075)  validation loss:  tensor(0.0075)\n",
      "r2:  -0.031330303349644595\n",
      "best validation loss:  tensor(0.0075)  validation loss:  tensor(0.0075)\n",
      "r2:  -0.029436500809747024\n",
      "best validation loss:  tensor(0.0075)  validation loss:  tensor(0.0075)\n",
      "r2:  -0.027580770806802102\n",
      "best validation loss:  tensor(0.0074)  validation loss:  tensor(0.0074)\n",
      "r2:  -0.025762273456113594\n",
      "best validation loss:  tensor(0.0074)  validation loss:  tensor(0.0074)\n",
      "r2:  -0.023980281367548262\n",
      "best validation loss:  tensor(0.0074)  validation loss:  tensor(0.0074)\n",
      "r2:  -0.022233971418698317\n",
      "best validation loss:  tensor(0.0074)  validation loss:  tensor(0.0074)\n",
      "r2:  -0.02052221946531607\n",
      "best validation loss:  tensor(0.0074)  validation loss:  tensor(0.0074)\n",
      "r2:  -0.018844143892800203\n",
      "best validation loss:  tensor(0.0074)  validation loss:  tensor(0.0074)\n",
      "r2:  -0.017199181173212397\n",
      "best validation loss:  tensor(0.0074)  validation loss:  tensor(0.0074)\n",
      "r2:  -0.015586608293194582\n",
      "best validation loss:  tensor(0.0074)  validation loss:  tensor(0.0074)\n",
      "r2:  -0.014005471233172528\n",
      "best validation loss:  tensor(0.0074)  validation loss:  tensor(0.0074)\n",
      "r2:  -0.012455035992366525\n",
      "best validation loss:  tensor(0.0073)  validation loss:  tensor(0.0073)\n",
      "r2:  -0.01093451478166485\n",
      "best validation loss:  tensor(0.0073)  validation loss:  tensor(0.0073)\n",
      "r2:  -0.009442894089129572\n",
      "best validation loss:  tensor(0.0073)  validation loss:  tensor(0.0073)\n",
      "r2:  -0.007979282812103472\n",
      "best validation loss:  tensor(0.0073)  validation loss:  tensor(0.0073)\n",
      "r2:  -0.006543088463459412\n",
      "best validation loss:  tensor(0.0073)  validation loss:  tensor(0.0073)\n",
      "r2:  -0.005133451289301316\n",
      "best validation loss:  tensor(0.0073)  validation loss:  tensor(0.0073)\n",
      "r2:  -0.0037498317702302497\n",
      "best validation loss:  tensor(0.0073)  validation loss:  tensor(0.0073)\n",
      "r2:  -0.002391136269223937\n",
      "best validation loss:  tensor(0.0073)  validation loss:  tensor(0.0073)\n",
      "r2:  -0.001056602267804685\n",
      "best validation loss:  tensor(0.0073)  validation loss:  tensor(0.0073)\n",
      "r2:  0.00025431523799213096\n",
      "best validation loss:  tensor(0.0072)  validation loss:  tensor(0.0072)\n",
      "r2:  0.0015425362790446595\n",
      "best validation loss:  tensor(0.0072)  validation loss:  tensor(0.0072)\n",
      "r2:  0.00280874349563931\n",
      "best validation loss:  tensor(0.0072)  validation loss:  tensor(0.0072)\n",
      "r2:  0.004053921802598115\n",
      "best validation loss:  tensor(0.0072)  validation loss:  tensor(0.0072)\n",
      "r2:  0.0052787893351338155\n",
      "best validation loss:  tensor(0.0072)  validation loss:  tensor(0.0072)\n",
      "r2:  0.006483999165626031\n",
      "best validation loss:  tensor(0.0072)  validation loss:  tensor(0.0072)\n",
      "r2:  0.007670162716374329\n",
      "best validation loss:  tensor(0.0072)  validation loss:  tensor(0.0072)\n",
      "r2:  0.008837751735582766\n",
      "best validation loss:  tensor(0.0072)  validation loss:  tensor(0.0072)\n",
      "r2:  0.00998719639416834\n",
      "best validation loss:  tensor(0.0072)  validation loss:  tensor(0.0072)\n",
      "r2:  0.011119352811256533\n",
      "best validation loss:  tensor(0.0072)  validation loss:  tensor(0.0072)\n",
      "r2:  0.012235186322399172\n",
      "best validation loss:  tensor(0.0072)  validation loss:  tensor(0.0072)\n",
      "r2:  0.013335074012456039\n",
      "best validation loss:  tensor(0.0072)  validation loss:  tensor(0.0072)\n",
      "r2:  0.014420196213233094\n",
      "best validation loss:  tensor(0.0071)  validation loss:  tensor(0.0071)\n",
      "r2:  0.015491312656068401\n",
      "best validation loss:  tensor(0.0071)  validation loss:  tensor(0.0071)\n",
      "r2:  0.01654859317657087\n",
      "best validation loss:  tensor(0.0071)  validation loss:  tensor(0.0071)\n",
      "r2:  0.01759251375744675\n",
      "best validation loss:  tensor(0.0071)  validation loss:  tensor(0.0071)\n",
      "r2:  0.018623759629084247\n",
      "best validation loss:  tensor(0.0071)  validation loss:  tensor(0.0071)\n",
      "r2:  0.019643261176748084\n",
      "best validation loss:  tensor(0.0071)  validation loss:  tensor(0.0071)\n",
      "r2:  0.020651802775553674\n",
      "best validation loss:  tensor(0.0071)  validation loss:  tensor(0.0071)\n",
      "r2:  0.021649682257944836\n",
      "best validation loss:  tensor(0.0071)  validation loss:  tensor(0.0071)\n",
      "r2:  0.022637697043657035\n",
      "best validation loss:  tensor(0.0071)  validation loss:  tensor(0.0071)\n",
      "r2:  0.02361668012726137\n",
      "best validation loss:  tensor(0.0071)  validation loss:  tensor(0.0071)\n",
      "r2:  0.024587014525909834\n",
      "best validation loss:  tensor(0.0071)  validation loss:  tensor(0.0071)\n",
      "r2:  0.025549230258039346\n",
      "best validation loss:  tensor(0.0071)  validation loss:  tensor(0.0071)\n",
      "r2:  0.026503877907466\n",
      "best validation loss:  tensor(0.0071)  validation loss:  tensor(0.0071)\n",
      "r2:  0.02745201431501465\n",
      "best validation loss:  tensor(0.0070)  validation loss:  tensor(0.0070)\n",
      "r2:  0.0283940478644098\n",
      "best validation loss:  tensor(0.0070)  validation loss:  tensor(0.0070)\n",
      "r2:  0.02933086059126011\n",
      "best validation loss:  tensor(0.0070)  validation loss:  tensor(0.0070)\n",
      "r2:  0.03026254590697197\n",
      "best validation loss:  tensor(0.0070)  validation loss:  tensor(0.0070)\n",
      "r2:  0.031190095150721797\n",
      "best validation loss:  tensor(0.0070)  validation loss:  tensor(0.0070)\n",
      "r2:  0.032113541403108736\n",
      "best validation loss:  tensor(0.0070)  validation loss:  tensor(0.0070)\n",
      "r2:  0.033033789751077625\n",
      "best validation loss:  tensor(0.0070)  validation loss:  tensor(0.0070)\n",
      "r2:  0.03395177964634766\n",
      "best validation loss:  tensor(0.0070)  validation loss:  tensor(0.0070)\n",
      "r2:  0.03486805132794035\n",
      "best validation loss:  tensor(0.0070)  validation loss:  tensor(0.0070)\n",
      "r2:  0.03578342528827083\n",
      "best validation loss:  tensor(0.0070)  validation loss:  tensor(0.0070)\n",
      "r2:  0.036697635838769616\n",
      "best validation loss:  tensor(0.0070)  validation loss:  tensor(0.0070)\n",
      "r2:  0.0376117872483136\n",
      "best validation loss:  tensor(0.0070)  validation loss:  tensor(0.0070)\n",
      "r2:  0.03852639399456703\n",
      "best validation loss:  tensor(0.0070)  validation loss:  tensor(0.0070)\n",
      "r2:  0.039441598888126705\n",
      "best validation loss:  tensor(0.0070)  validation loss:  tensor(0.0070)\n",
      "r2:  0.04035826294662925\n",
      "best validation loss:  tensor(0.0070)  validation loss:  tensor(0.0070)\n",
      "r2:  0.04127667792134171\n",
      "best validation loss:  tensor(0.0069)  validation loss:  tensor(0.0069)\n",
      "r2:  0.04219848964721684\n",
      "best validation loss:  tensor(0.0069)  validation loss:  tensor(0.0069)\n",
      "r2:  0.043123502582967155\n",
      "best validation loss:  tensor(0.0069)  validation loss:  tensor(0.0069)\n",
      "r2:  0.04405134620869533\n",
      "best validation loss:  tensor(0.0069)  validation loss:  tensor(0.0069)\n",
      "r2:  0.044983426694221285\n",
      "best validation loss:  tensor(0.0069)  validation loss:  tensor(0.0069)\n",
      "r2:  0.04592086526180428\n",
      "best validation loss:  tensor(0.0069)  validation loss:  tensor(0.0069)\n",
      "r2:  0.04686286973342302\n",
      "best validation loss:  tensor(0.0069)  validation loss:  tensor(0.0069)\n",
      "r2:  0.0478109173836877\n",
      "best validation loss:  tensor(0.0069)  validation loss:  tensor(0.0069)\n",
      "r2:  0.048764762605233325\n",
      "best validation loss:  tensor(0.0069)  validation loss:  tensor(0.0069)\n",
      "r2:  0.049725548197762426\n",
      "best validation loss:  tensor(0.0069)  validation loss:  tensor(0.0069)\n",
      "r2:  0.05069600732068469\n",
      "best validation loss:  tensor(0.0069)  validation loss:  tensor(0.0069)\n",
      "r2:  0.05167488953069965\n",
      "best validation loss:  tensor(0.0069)  validation loss:  tensor(0.0069)\n",
      "r2:  0.05266240048740489\n",
      "best validation loss:  tensor(0.0069)  validation loss:  tensor(0.0069)\n",
      "r2:  0.053657982157320094\n",
      "best validation loss:  tensor(0.0069)  validation loss:  tensor(0.0069)\n",
      "r2:  0.054662747573251624\n",
      "best validation loss:  tensor(0.0069)  validation loss:  tensor(0.0069)\n",
      "r2:  0.055677319143698\n",
      "best validation loss:  tensor(0.0068)  validation loss:  tensor(0.0068)\n",
      "r2:  0.05670090844321207\n",
      "best validation loss:  tensor(0.0068)  validation loss:  tensor(0.0068)\n",
      "r2:  0.05773317791835675\n",
      "best validation loss:  tensor(0.0068)  validation loss:  tensor(0.0068)\n",
      "r2:  0.058774554803150636\n",
      "best validation loss:  tensor(0.0068)  validation loss:  tensor(0.0068)\n",
      "r2:  0.05982560977037388\n",
      "best validation loss:  tensor(0.0068)  validation loss:  tensor(0.0068)\n",
      "r2:  0.06088660019913539\n",
      "best validation loss:  tensor(0.0068)  validation loss:  tensor(0.0068)\n",
      "r2:  0.06195775265978054\n",
      "best validation loss:  tensor(0.0068)  validation loss:  tensor(0.0068)\n",
      "r2:  0.06303985647017252\n",
      "best validation loss:  tensor(0.0068)  validation loss:  tensor(0.0068)\n",
      "r2:  0.064133728735143\n",
      "best validation loss:  tensor(0.0068)  validation loss:  tensor(0.0068)\n",
      "r2:  0.06523710662116544\n",
      "best validation loss:  tensor(0.0068)  validation loss:  tensor(0.0068)\n",
      "r2:  0.0663495671918144\n",
      "best validation loss:  tensor(0.0068)  validation loss:  tensor(0.0068)\n",
      "r2:  0.06747195645359594\n",
      "best validation loss:  tensor(0.0068)  validation loss:  tensor(0.0068)\n",
      "r2:  0.06860207514379857\n",
      "best validation loss:  tensor(0.0068)  validation loss:  tensor(0.0068)\n",
      "r2:  0.06973950254415517\n",
      "best validation loss:  tensor(0.0067)  validation loss:  tensor(0.0067)\n",
      "r2:  0.07088360822543582\n",
      "best validation loss:  tensor(0.0067)  validation loss:  tensor(0.0067)\n",
      "r2:  0.07203417126148814\n",
      "best validation loss:  tensor(0.0067)  validation loss:  tensor(0.0067)\n",
      "r2:  0.0731903144025956\n",
      "best validation loss:  tensor(0.0067)  validation loss:  tensor(0.0067)\n",
      "r2:  0.07435079947378576\n",
      "best validation loss:  tensor(0.0067)  validation loss:  tensor(0.0067)\n",
      "r2:  0.07551430557835181\n",
      "best validation loss:  tensor(0.0067)  validation loss:  tensor(0.0067)\n",
      "r2:  0.07668371839470545\n",
      "best validation loss:  tensor(0.0067)  validation loss:  tensor(0.0067)\n",
      "r2:  0.07785922669214018\n",
      "best validation loss:  tensor(0.0067)  validation loss:  tensor(0.0067)\n",
      "r2:  0.07903994355102995\n",
      "best validation loss:  tensor(0.0067)  validation loss:  tensor(0.0067)\n",
      "r2:  0.08022616314566211\n",
      "best validation loss:  tensor(0.0067)  validation loss:  tensor(0.0067)\n",
      "r2:  0.08141679936086843\n",
      "best validation loss:  tensor(0.0067)  validation loss:  tensor(0.0067)\n",
      "r2:  0.0826105466521253\n",
      "best validation loss:  tensor(0.0067)  validation loss:  tensor(0.0067)\n",
      "r2:  0.08380741905891242\n",
      "best validation loss:  tensor(0.0066)  validation loss:  tensor(0.0066)\n",
      "r2:  0.08500826288518037\n",
      "best validation loss:  tensor(0.0066)  validation loss:  tensor(0.0066)\n",
      "r2:  0.08621234687305657\n",
      "best validation loss:  tensor(0.0066)  validation loss:  tensor(0.0066)\n",
      "r2:  0.08741870831508791\n",
      "best validation loss:  tensor(0.0066)  validation loss:  tensor(0.0066)\n",
      "r2:  0.08862778143634775\n",
      "best validation loss:  tensor(0.0066)  validation loss:  tensor(0.0066)\n",
      "r2:  0.08983874662536873\n",
      "best validation loss:  tensor(0.0066)  validation loss:  tensor(0.0066)\n",
      "r2:  0.09105432599417174\n",
      "best validation loss:  tensor(0.0066)  validation loss:  tensor(0.0066)\n",
      "r2:  0.09227415691888985\n",
      "best validation loss:  tensor(0.0066)  validation loss:  tensor(0.0066)\n",
      "r2:  0.09349758512798467\n",
      "best validation loss:  tensor(0.0066)  validation loss:  tensor(0.0066)\n",
      "r2:  0.09472545843019775\n",
      "best validation loss:  tensor(0.0066)  validation loss:  tensor(0.0066)\n",
      "r2:  0.09595799276374306\n",
      "best validation loss:  tensor(0.0066)  validation loss:  tensor(0.0066)\n",
      "r2:  0.09719554297842503\n",
      "best validation loss:  tensor(0.0065)  validation loss:  tensor(0.0065)\n",
      "r2:  0.09843868017656132\n",
      "best validation loss:  tensor(0.0065)  validation loss:  tensor(0.0065)\n",
      "r2:  0.09968697850826935\n",
      "best validation loss:  tensor(0.0065)  validation loss:  tensor(0.0065)\n",
      "r2:  0.10094041597448711\n",
      "best validation loss:  tensor(0.0065)  validation loss:  tensor(0.0065)\n",
      "r2:  0.10219841038031086\n",
      "best validation loss:  tensor(0.0065)  validation loss:  tensor(0.0065)\n",
      "r2:  0.10346194146995169\n",
      "best validation loss:  tensor(0.0065)  validation loss:  tensor(0.0065)\n",
      "r2:  0.10473125323992982\n",
      "best validation loss:  tensor(0.0065)  validation loss:  tensor(0.0065)\n",
      "r2:  0.1060078097602194\n",
      "best validation loss:  tensor(0.0065)  validation loss:  tensor(0.0065)\n",
      "r2:  0.10729333950866682\n",
      "best validation loss:  tensor(0.0065)  validation loss:  tensor(0.0065)\n",
      "r2:  0.10858646728562182\n",
      "best validation loss:  tensor(0.0065)  validation loss:  tensor(0.0065)\n",
      "r2:  0.10988839743186574\n",
      "best validation loss:  tensor(0.0065)  validation loss:  tensor(0.0065)\n",
      "r2:  0.11119745308380558\n",
      "best validation loss:  tensor(0.0064)  validation loss:  tensor(0.0064)\n",
      "r2:  0.11251448435471201\n",
      "best validation loss:  tensor(0.0064)  validation loss:  tensor(0.0064)\n",
      "r2:  0.11384052875051387\n",
      "best validation loss:  tensor(0.0064)  validation loss:  tensor(0.0064)\n",
      "r2:  0.11517660926528206\n",
      "best validation loss:  tensor(0.0064)  validation loss:  tensor(0.0064)\n",
      "r2:  0.11652220440827932\n",
      "best validation loss:  tensor(0.0064)  validation loss:  tensor(0.0064)\n",
      "r2:  0.11787780219792965\n",
      "best validation loss:  tensor(0.0064)  validation loss:  tensor(0.0064)\n",
      "r2:  0.11924184717439879\n",
      "best validation loss:  tensor(0.0064)  validation loss:  tensor(0.0064)\n",
      "r2:  0.12061505988995669\n",
      "best validation loss:  tensor(0.0064)  validation loss:  tensor(0.0064)\n",
      "r2:  0.12199925225133446\n",
      "best validation loss:  tensor(0.0064)  validation loss:  tensor(0.0064)\n",
      "r2:  0.1233934452705715\n",
      "best validation loss:  tensor(0.0064)  validation loss:  tensor(0.0064)\n",
      "r2:  0.12479799423044191\n",
      "best validation loss:  tensor(0.0063)  validation loss:  tensor(0.0063)\n",
      "r2:  0.1262148285616227\n",
      "best validation loss:  tensor(0.0063)  validation loss:  tensor(0.0063)\n",
      "r2:  0.1276446048121258\n",
      "best validation loss:  tensor(0.0063)  validation loss:  tensor(0.0063)\n",
      "r2:  0.1290855224358306\n",
      "best validation loss:  tensor(0.0063)  validation loss:  tensor(0.0063)\n",
      "r2:  0.1305392508392339\n",
      "best validation loss:  tensor(0.0063)  validation loss:  tensor(0.0063)\n",
      "r2:  0.13200509647183656\n",
      "best validation loss:  tensor(0.0063)  validation loss:  tensor(0.0063)\n",
      "r2:  0.1334848561492734\n",
      "best validation loss:  tensor(0.0063)  validation loss:  tensor(0.0063)\n",
      "r2:  0.13497809782996406\n",
      "best validation loss:  tensor(0.0063)  validation loss:  tensor(0.0063)\n",
      "r2:  0.13648306915824437\n",
      "best validation loss:  tensor(0.0063)  validation loss:  tensor(0.0063)\n",
      "r2:  0.13799932939055237\n",
      "best validation loss:  tensor(0.0062)  validation loss:  tensor(0.0062)\n",
      "r2:  0.13952944612570684\n",
      "best validation loss:  tensor(0.0062)  validation loss:  tensor(0.0062)\n",
      "r2:  0.1410749986745261\n",
      "best validation loss:  tensor(0.0062)  validation loss:  tensor(0.0062)\n",
      "r2:  0.14263764813932522\n",
      "best validation loss:  tensor(0.0062)  validation loss:  tensor(0.0062)\n",
      "r2:  0.14421428653051682\n",
      "best validation loss:  tensor(0.0062)  validation loss:  tensor(0.0062)\n",
      "r2:  0.14580341893508775\n",
      "best validation loss:  tensor(0.0062)  validation loss:  tensor(0.0062)\n",
      "r2:  0.1474065526196633\n",
      "best validation loss:  tensor(0.0062)  validation loss:  tensor(0.0062)\n",
      "r2:  0.1490245014939635\n",
      "best validation loss:  tensor(0.0062)  validation loss:  tensor(0.0062)\n",
      "r2:  0.15065726245046707\n",
      "best validation loss:  tensor(0.0062)  validation loss:  tensor(0.0062)\n",
      "r2:  0.152298685089701\n",
      "best validation loss:  tensor(0.0061)  validation loss:  tensor(0.0061)\n",
      "r2:  0.1539530539009255\n",
      "best validation loss:  tensor(0.0061)  validation loss:  tensor(0.0061)\n",
      "r2:  0.1556199704066824\n",
      "best validation loss:  tensor(0.0061)  validation loss:  tensor(0.0061)\n",
      "r2:  0.1572957715514509\n",
      "best validation loss:  tensor(0.0061)  validation loss:  tensor(0.0061)\n",
      "r2:  0.1589753313756489\n",
      "best validation loss:  tensor(0.0061)  validation loss:  tensor(0.0061)\n",
      "r2:  0.16065824577588395\n",
      "best validation loss:  tensor(0.0061)  validation loss:  tensor(0.0061)\n",
      "r2:  0.1623506292090895\n",
      "best validation loss:  tensor(0.0061)  validation loss:  tensor(0.0061)\n",
      "r2:  0.16404962862357042\n",
      "best validation loss:  tensor(0.0061)  validation loss:  tensor(0.0061)\n",
      "r2:  0.1657562897980427\n",
      "best validation loss:  tensor(0.0060)  validation loss:  tensor(0.0060)\n",
      "r2:  0.16746480599184976\n",
      "best validation loss:  tensor(0.0060)  validation loss:  tensor(0.0060)\n",
      "r2:  0.16917494051334003\n",
      "best validation loss:  tensor(0.0060)  validation loss:  tensor(0.0060)\n",
      "r2:  0.17088810109704844\n",
      "best validation loss:  tensor(0.0060)  validation loss:  tensor(0.0060)\n",
      "r2:  0.17260434697254146\n",
      "best validation loss:  tensor(0.0060)  validation loss:  tensor(0.0060)\n",
      "r2:  0.17432246850999844\n",
      "best validation loss:  tensor(0.0060)  validation loss:  tensor(0.0060)\n",
      "r2:  0.17604470799503635\n",
      "best validation loss:  tensor(0.0060)  validation loss:  tensor(0.0060)\n",
      "r2:  0.17776140806838792\n",
      "best validation loss:  tensor(0.0060)  validation loss:  tensor(0.0060)\n",
      "r2:  0.1794763697649271\n",
      "best validation loss:  tensor(0.0059)  validation loss:  tensor(0.0059)\n",
      "r2:  0.1811917206677367\n",
      "best validation loss:  tensor(0.0059)  validation loss:  tensor(0.0059)\n",
      "r2:  0.18290343177470147\n",
      "best validation loss:  tensor(0.0059)  validation loss:  tensor(0.0059)\n",
      "r2:  0.1846107703880644\n",
      "best validation loss:  tensor(0.0059)  validation loss:  tensor(0.0059)\n",
      "r2:  0.18631204637837373\n",
      "best validation loss:  tensor(0.0059)  validation loss:  tensor(0.0059)\n",
      "r2:  0.1880040541842829\n",
      "best validation loss:  tensor(0.0059)  validation loss:  tensor(0.0059)\n",
      "r2:  0.18968979787464413\n",
      "best validation loss:  tensor(0.0059)  validation loss:  tensor(0.0059)\n",
      "r2:  0.19137010719144332\n",
      "best validation loss:  tensor(0.0059)  validation loss:  tensor(0.0059)\n",
      "r2:  0.19304454067332355\n",
      "best validation loss:  tensor(0.0058)  validation loss:  tensor(0.0058)\n",
      "r2:  0.19471351716079643\n",
      "best validation loss:  tensor(0.0058)  validation loss:  tensor(0.0058)\n",
      "r2:  0.19637680190397888\n",
      "best validation loss:  tensor(0.0058)  validation loss:  tensor(0.0058)\n",
      "r2:  0.1980345035806148\n",
      "best validation loss:  tensor(0.0058)  validation loss:  tensor(0.0058)\n",
      "r2:  0.19968515527740083\n",
      "best validation loss:  tensor(0.0058)  validation loss:  tensor(0.0058)\n",
      "r2:  0.20132907288581703\n",
      "best validation loss:  tensor(0.0058)  validation loss:  tensor(0.0058)\n",
      "r2:  0.20296505418741106\n",
      "best validation loss:  tensor(0.0058)  validation loss:  tensor(0.0058)\n",
      "r2:  0.20459323449004352\n",
      "best validation loss:  tensor(0.0058)  validation loss:  tensor(0.0058)\n",
      "r2:  0.20621407469188413\n",
      "best validation loss:  tensor(0.0058)  validation loss:  tensor(0.0058)\n",
      "r2:  0.20782648177071716\n",
      "best validation loss:  tensor(0.0057)  validation loss:  tensor(0.0057)\n",
      "r2:  0.20943204080473132\n",
      "best validation loss:  tensor(0.0057)  validation loss:  tensor(0.0057)\n",
      "r2:  0.2110325461738921\n",
      "best validation loss:  tensor(0.0057)  validation loss:  tensor(0.0057)\n",
      "r2:  0.2126261441757592\n",
      "best validation loss:  tensor(0.0057)  validation loss:  tensor(0.0057)\n",
      "r2:  0.21421160159792707\n",
      "best validation loss:  tensor(0.0057)  validation loss:  tensor(0.0057)\n",
      "r2:  0.21578884953757405\n",
      "best validation loss:  tensor(0.0057)  validation loss:  tensor(0.0057)\n",
      "r2:  0.21735860142069752\n",
      "best validation loss:  tensor(0.0057)  validation loss:  tensor(0.0057)\n",
      "r2:  0.21892046987738012\n",
      "best validation loss:  tensor(0.0057)  validation loss:  tensor(0.0057)\n",
      "r2:  0.22047567426094294\n",
      "best validation loss:  tensor(0.0057)  validation loss:  tensor(0.0057)\n",
      "r2:  0.22202272591146577\n",
      "best validation loss:  tensor(0.0056)  validation loss:  tensor(0.0056)\n",
      "r2:  0.22356090160694697\n",
      "best validation loss:  tensor(0.0056)  validation loss:  tensor(0.0056)\n",
      "r2:  0.22509145363593996\n",
      "best validation loss:  tensor(0.0056)  validation loss:  tensor(0.0056)\n",
      "r2:  0.22661434223957444\n",
      "best validation loss:  tensor(0.0056)  validation loss:  tensor(0.0056)\n",
      "r2:  0.2281290301593143\n",
      "best validation loss:  tensor(0.0056)  validation loss:  tensor(0.0056)\n",
      "r2:  0.22963586050280949\n",
      "best validation loss:  tensor(0.0056)  validation loss:  tensor(0.0056)\n",
      "r2:  0.2311352428857567\n",
      "best validation loss:  tensor(0.0056)  validation loss:  tensor(0.0056)\n",
      "r2:  0.23262562617358995\n",
      "best validation loss:  tensor(0.0056)  validation loss:  tensor(0.0056)\n",
      "r2:  0.23410795937036422\n",
      "best validation loss:  tensor(0.0056)  validation loss:  tensor(0.0056)\n",
      "r2:  0.2355827397174015\n",
      "best validation loss:  tensor(0.0055)  validation loss:  tensor(0.0055)\n",
      "r2:  0.2370496477028945\n",
      "best validation loss:  tensor(0.0055)  validation loss:  tensor(0.0055)\n",
      "r2:  0.23850830170618287\n",
      "best validation loss:  tensor(0.0055)  validation loss:  tensor(0.0055)\n",
      "r2:  0.23995935715914563\n",
      "best validation loss:  tensor(0.0055)  validation loss:  tensor(0.0055)\n",
      "r2:  0.24140251954446945\n",
      "best validation loss:  tensor(0.0055)  validation loss:  tensor(0.0055)\n",
      "r2:  0.24283718685621025\n",
      "best validation loss:  tensor(0.0055)  validation loss:  tensor(0.0055)\n",
      "r2:  0.24426379438024015\n",
      "best validation loss:  tensor(0.0055)  validation loss:  tensor(0.0055)\n",
      "r2:  0.24568213713672726\n",
      "best validation loss:  tensor(0.0055)  validation loss:  tensor(0.0055)\n",
      "r2:  0.2470932937551661\n",
      "best validation loss:  tensor(0.0055)  validation loss:  tensor(0.0055)\n",
      "r2:  0.24849714631287667\n",
      "best validation loss:  tensor(0.0054)  validation loss:  tensor(0.0054)\n",
      "r2:  0.2498935379991245\n",
      "best validation loss:  tensor(0.0054)  validation loss:  tensor(0.0054)\n",
      "r2:  0.251282165411302\n",
      "best validation loss:  tensor(0.0054)  validation loss:  tensor(0.0054)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 41\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     40\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 41\u001b[0m     outputs \u001b[39m=\u001b[39m model(tensor_x_training)\n\u001b[1;32m     42\u001b[0m     loss \u001b[39m=\u001b[39m loss_fn(outputs, tensor_y_training\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m))\n\u001b[1;32m     43\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "from torchsummary import summary\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score\n",
    "import torch.optim as optim\n",
    "# import time\n",
    "from time import time\n",
    "\n",
    "parameters = [8, 16, 32, 64]\n",
    "learning_rates = [0.0001, 0.001, 0.01]\n",
    "\n",
    "r2_min = {\"r2\": -1, \"parameter\": 0, \"learning_rate\": 0}\n",
    "\n",
    "for parameter in parameters:\n",
    "    for learning_rate in learning_rates:\n",
    "        \n",
    "        print(\"testing out parameter: \", parameter, \" and learning rate: \", learning_rate)\n",
    "        print(\"current r2 min : \", r2_min)\n",
    "        \n",
    "        # start timer\n",
    "        start = time.time()\n",
    "\n",
    "        model = nn.Sequential(\n",
    "\n",
    "            nn.Linear(tensor_x_training.shape[1], parameter),\n",
    "\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(parameter, parameter),\n",
    "\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(parameter, parameter),\n",
    "\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(parameter, 1),\n",
    "\n",
    "        )\n",
    "\n",
    "        loss_fn = nn.MSELoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "        best_validation_loss = float('inf')\n",
    "\n",
    "        while True:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(tensor_x_training)\n",
    "            loss = loss_fn(outputs, tensor_y_training.unsqueeze(1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Calculate validation loss\n",
    "            with torch.no_grad():\n",
    "                validation_outputs = model(tensor_x_validation)\n",
    "                validation_loss = loss_fn(validation_outputs, tensor_y_validation.unsqueeze(1))\n",
    "\n",
    "                r2 = r2_score(tensor_y_validation.numpy(), validation_outputs.numpy())\n",
    "\n",
    "                if r2 > r2_min[\"r2\"]:\n",
    "                    r2_min[\"r2\"] = r2\n",
    "                    r2_min[\"parameter\"] = parameter\n",
    "                    r2_min[\"learning_rate\"] = learning_rate\n",
    "\n",
    "            # check for early stopping\n",
    "            if validation_loss >= best_validation_loss:\n",
    "                break \n",
    "            \n",
    "            # stop if the timer has reached 9 minutes\n",
    "            if time.time() - start > 540:\n",
    "                print(\"timer has reached 9 minutes\")\n",
    "                print(\"r2 min : \", r2)\n",
    "                break\n",
    "\n",
    "            best_validation_loss = validation_loss\n",
    "            \n",
    "            print(\"r2: \", r2)\n",
    "            print(\"best validation loss: \", best_validation_loss, \" validation loss: \", validation_loss)\n",
    "\n",
    "print(\"r2 min : \", r2_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = nn.Sequential(\n",
    "\n",
    "#     nn.Linear(tensor_x_training.shape[1], parameter),\n",
    "\n",
    "#     nn.ReLU(),\n",
    "#     nn.Linear(parameter, parameter),\n",
    "\n",
    "#     nn.ReLU(),\n",
    "#     nn.Linear(parameter, 1),\n",
    "# )\n",
    "\n",
    "# after 23 minutes, 8 parameters\n",
    "# r2:  0.9022029945690702\n",
    "# best validation loss:  tensor(0.0007)  validation loss:  tensor(0.0007)\n",
    "\n",
    "# after 13 minutes, 16 parameters\n",
    "# r2:  0.9024288425404208\n",
    "# best validation loss:  tensor(0.0007)  validation loss:  tensor(0.0007)\n",
    "\n",
    "# after 9 minutes, 32 parameters\n",
    "# r2:  0.9110110570893579\n",
    "# best validation loss:  tensor(0.0006)  validation loss:  tensor(0.0006)\n",
    "\n",
    "# after 9 minutes, 64 parameters\n",
    "#r2:  0.8424793580733347\n",
    "#best validation loss:  tensor(0.0011)  validation loss:  tensor(0.0011)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = nn.Sequential(\n",
    "\n",
    "#     nn.Linear(tensor_x_training.shape[1], parameter),\n",
    "\n",
    "#     nn.ReLU(),\n",
    "#     nn.Linear(parameter, parameter),\n",
    "\n",
    "#     nn.ReLU(),\n",
    "#     nn.Linear(parameter, parameter),\n",
    "\n",
    "#     nn.ReLU(),\n",
    "#     nn.Linear(parameter, 1),\n",
    "\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
